{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamonaChristen/Multilingual_Negation_Scope_Resolution_on_Legal_Data/blob/main/Multilingual_Negation_Scope_Resolution_on_Legal_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The base for this code was copied from [Transformers_for_Negation_and_Speculation](https://github.com/adityak6798/Transformers-For-Negation-and-Speculation/blob/master/Transformers_for_Negation_and_Speculation.ipynb)"
      ],
      "metadata": {
        "id": "OADYQ1QYs9WV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz4F2kYxGF3f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install knockknock==0.1.7\n",
        "!pip install keras_preprocessing\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ujGlqJGTfc"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os, re, torch, html, tempfile, copy, json, math, shutil, tarfile, tempfile, sys, random, pickle\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import CrossEntropyLoss, ReLU\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from transformers import AutoModelForTokenClassification,AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from scipy import stats\n",
        "import csv\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7wJf3tYGwmD"
      },
      "outputs": [],
      "source": [
        "#GLOBAL VARIABLEs\n",
        "MAX_LEN = 256\n",
        "bs = 16\n",
        "EPOCHS = 32\n",
        "PATIENCE = 8\n",
        "INITIAL_LEARNING_RATE = 1e-5\n",
        "\n",
        "SCOPE_METHOD = 'augment' # Options: augment, replace\n",
        "F1_METHOD = 'average' # Options: average, first_token\n",
        "TASK = 'negation'\n",
        "SUBTASK = 'scope_resolution'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq92H2JlGzvC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "n_gpu = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJeHety_G2oj"
      },
      "outputs": [],
      "source": [
        "class Cues:\n",
        "    def __init__(self, data):\n",
        "        self.sentences = data[0]\n",
        "        self.cues = data[1]\n",
        "        self.num_sentences = len(data[0])\n",
        "class Scopes:\n",
        "    def __init__(self, data):\n",
        "        self.sentences = data[0]\n",
        "        self.cues = data[1]\n",
        "        self.scopes = data[2]\n",
        "        self.num_sentences = len(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq07VC_fG6cm"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def append(self, new_data):\n",
        "      for d in new_data:\n",
        "        self.cue_data.sentences += d.cue_data.sentences\n",
        "        self.cue_data.cues += d.cue_data.cues\n",
        "        self.cue_data.num_sentences += d.cue_data.num_sentences\n",
        "        self.scope_data.sentences += d.scope_data.sentences\n",
        "        self.scope_data.cues += d.scope_data.cues\n",
        "        self.scope_data.scopes += d.scope_data.scopes\n",
        "        self.scope_data.num_sentences += d.scope_data.num_sentences\n",
        "\n",
        "\n",
        "    def __init__(self, data, dataset_name = 'sfu', frac_no_cue_sents = 1.0):\n",
        "        self.dataset_name = dataset_name\n",
        "        '''\n",
        "        file: The path of the data file.\n",
        "        dataset_name: The name of the dataset to be preprocessed. Values supported: sfu, bioscope, starsem.\n",
        "        frac_no_cue_sents: The fraction of sentences to be included in the data object which have no negation/speculation cues.\n",
        "        '''\n",
        "\n",
        "        cue_sentence = []\n",
        "        cue_cues = []\n",
        "        no_cue_data = []\n",
        "        scope_cues = []\n",
        "        scope_scopes = []\n",
        "        scope_sentence = []\n",
        "        sentence = []\n",
        "        in_scope = []\n",
        "        in_cue = []\n",
        "        word_num = 0\n",
        "        c_idx = []\n",
        "        s_idx = []\n",
        "        for line in data:\n",
        "          sentence = []\n",
        "          for token in line['tokens']:\n",
        "            sentence.append(token['text'])\n",
        "          if not line['spans']:\n",
        "            no_cue_data.append([sentence,[3]*len(sentence)])\n",
        "          elif len(line['tokens'])>0:\n",
        "            scopes = []\n",
        "            cues = []\n",
        "            for span in line['spans']:\n",
        "              if span['label'] == 'SCOPE':\n",
        "                scopes.append(span)\n",
        "              elif span['label'] == 'CUE':\n",
        "                cues.append(span)\n",
        "            cue_sentence.append(sentence)\n",
        "            scope_sentence.append(sentence)\n",
        "            cue_cues.append([3]*len(sentence))\n",
        "            scope_cues.append([3]*len(sentence))\n",
        "            scope_scopes.append([0]*len(sentence))\n",
        "            if len(cues) == 1:\n",
        "              cue = cues[0]\n",
        "              c_idx = np.arange(cue['token_start'],cue['token_end']+1)\n",
        "              if len(c_idx) == 1:\n",
        "                for c in c_idx:\n",
        "                  cue_cues[-1][c] = 1\n",
        "                  scope_cues[-1][c] = 1\n",
        "              elif len(c_idx) >1:\n",
        "                for c in c_idx:\n",
        "                  cue_cues[-1][c] = 2\n",
        "                  scope_cues[-1][c] = 2\n",
        "            elif len(cues)>1:\n",
        "              for cue in cues:\n",
        "                c_idx = np.arange(cue['token_start'],cue['token_end']+1)\n",
        "                for c in c_idx:\n",
        "                  cue_cues[-1][c] = 2\n",
        "                  scope_cues[-1][c] = 2\n",
        "            for scope in scopes:\n",
        "              s_idx = np.arange(scope['token_start'],scope['token_end']+1)\n",
        "              for s in s_idx:\n",
        "                scope_scopes[-1][s] = 1\n",
        "            for idx, c in enumerate(cue_cues[-1]):\n",
        "                  if c == 1 and scope_scopes[-1][idx] == 1:\n",
        "                    cue_cues[-1][idx] = 0\n",
        "                    scope_cues[-1][idx] = 0\n",
        "        cue_only_samples = random.sample(no_cue_data, k=int(frac_no_cue_sents*len(no_cue_data)))\n",
        "        cue_only_sents = [i[0] for i in cue_only_samples]\n",
        "        cue_only_cues = [i[1] for i in cue_only_samples]\n",
        "\n",
        "        self.cue_data = Cues((cue_sentence+cue_only_sents, cue_cues+cue_only_cues))\n",
        "        self.scope_data = Scopes((scope_sentence, scope_cues, scope_scopes))\n",
        "\n",
        "    def get_scope_dataloader(self, val_size = 0.15, test_size=0.15, other_datasets = [], combine = True):\n",
        "        '''\n",
        "        This function returns the dataloader for the cue detection.\n",
        "        val_size: The size of the validation dataset (Fraction between 0 to 1)\n",
        "        test_size: The size of the test dataset (Fraction between 0 to 1)\n",
        "        other_datasets: Other datasets to use to get one combined train dataloader\n",
        "        Returns: train_dataloader, list of validation dataloaders, list of test dataloaders\n",
        "        '''\n",
        "        method = SCOPE_METHOD\n",
        "        do_lower_case = True\n",
        "        if 'uncased' not in SCOPE_MODEL and 'cased' in SCOPE_MODEL:\n",
        "            do_lower_case = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(SCOPE_MODEL, do_lower_case=do_lower_case)\n",
        "\n",
        "        def preprocess_data(obj):\n",
        "            dl_sents = obj.scope_data.sentences\n",
        "            dl_cues = obj.scope_data.cues\n",
        "            dl_scopes = obj.scope_data.scopes\n",
        "\n",
        "            sentences = [\" \".join([s for s in sent]) for sent in dl_sents]\n",
        "            mytexts = []\n",
        "            mylabels = []\n",
        "            mycues = []\n",
        "            mymasks = []\n",
        "            if do_lower_case == True:\n",
        "                sentences_clean = [sent.lower() for sent in sentences]\n",
        "            else:\n",
        "                sentences_clean = sentences\n",
        "\n",
        "            for sent, tags, cues in zip(sentences_clean,dl_scopes, dl_cues):\n",
        "                new_tags = []\n",
        "                new_text = []\n",
        "                new_cues = []\n",
        "                new_masks = []\n",
        "                for word, tag, cue in zip(sent.split(),tags,cues):\n",
        "                    sub_words = self.tokenizer.tokenize(word)\n",
        "                    for count, sub_word in enumerate(sub_words):\n",
        "                        mask = 1\n",
        "                        if count > 0:\n",
        "                            mask = 0\n",
        "                        new_masks.append(mask)\n",
        "                        new_tags.append(tag)\n",
        "                        new_cues.append(cue)\n",
        "                        new_text.append(sub_word)\n",
        "                mymasks.append(new_masks)\n",
        "                mytexts.append(new_text)\n",
        "                mylabels.append(new_tags)\n",
        "                mycues.append(new_cues)\n",
        "            final_sentences = []\n",
        "            final_labels = []\n",
        "            final_masks = []\n",
        "            if method == 'replace':\n",
        "                for sent,cues in zip(mytexts, mycues):\n",
        "                    temp_sent = []\n",
        "                    for token,cue in zip(sent,cues):\n",
        "                        if cue==3:\n",
        "                            temp_sent.append(token)\n",
        "                        else:\n",
        "                            temp_sent.append(f'[unused{cue+1}]')\n",
        "                    final_sentences.append(temp_sent)\n",
        "                final_labels = mylabels\n",
        "                final_masks = mymasks\n",
        "            elif method == 'augment':\n",
        "                for sent,cues,labels,masks in zip(mytexts, mycues, mylabels, mymasks):\n",
        "                    temp_sent = []\n",
        "                    temp_label = []\n",
        "                    temp_masks = []\n",
        "                    first_part = 0\n",
        "                    for token,cue,label,mask in zip(sent,cues,labels,masks):\n",
        "                        if cue!=3:\n",
        "                            if first_part == 0:\n",
        "                                first_part = 1\n",
        "                                temp_sent.append(f'[unused{cue+1}]')\n",
        "                                temp_masks.append(1)\n",
        "                                temp_label.append(label)\n",
        "                                temp_sent.append(token)\n",
        "                                temp_masks.append(0)\n",
        "                                temp_label.append(label)\n",
        "                                continue\n",
        "                            temp_sent.append(f'[unused{cue+1}]')\n",
        "                            temp_masks.append(0)\n",
        "                            temp_label.append(label)\n",
        "                        else:\n",
        "                            first_part = 0\n",
        "                        temp_masks.append(mask)\n",
        "                        temp_sent.append(token)\n",
        "                        temp_label.append(label)\n",
        "                    final_sentences.append(temp_sent)\n",
        "                    final_labels.append(temp_label)\n",
        "                    final_masks.append(temp_masks)\n",
        "                    if (len(temp_sent) > MAX_LEN):\n",
        "                      print(\"***WARNING*** \\ninput over MAX LENGTH\")\n",
        "            else:\n",
        "                raise ValueError(\"Supported methods for scope detection are:\\nreplace\\naugment\")\n",
        "            input_ids = pad_sequences([[self.tokenizer.convert_tokens_to_ids(word) for word in txt] for txt in final_sentences],\n",
        "                                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\").tolist()\n",
        "\n",
        "            tags = pad_sequences(final_labels,\n",
        "                                maxlen=MAX_LEN, value=0, padding=\"post\",\n",
        "                                dtype=\"long\", truncating=\"post\").tolist()\n",
        "\n",
        "            final_masks = pad_sequences(final_masks,\n",
        "                                maxlen=MAX_LEN, value=0, padding=\"post\",\n",
        "                                dtype=\"long\", truncating=\"post\").tolist()\n",
        "\n",
        "            attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "\n",
        "            return [input_ids, tags, attention_masks, final_masks]\n",
        "\n",
        "        inputs = []\n",
        "        tags = []\n",
        "        masks = []\n",
        "        mymasks = []\n",
        "        ret_val = preprocess_data(self)\n",
        "        inputs.append(ret_val[0])\n",
        "        tags.append(ret_val[1])\n",
        "        masks.append(ret_val[2])\n",
        "        mymasks.append(ret_val[3])\n",
        "\n",
        "        for idx, arg in enumerate(other_datasets):\n",
        "            ret_val = preprocess_data(arg)\n",
        "            if(combine):\n",
        "                inputs[0]+=ret_val[0]\n",
        "                tags[0]+=ret_val[1]\n",
        "                masks[0]+=ret_val[2]\n",
        "                mymasks[0]+=ret_val[3]\n",
        "            else:\n",
        "                inputs.append(ret_val[0])\n",
        "                tags.append(ret_val[1])\n",
        "                masks.append(ret_val[2])\n",
        "                mymasks.append(ret_val[3])\n",
        "\n",
        "\n",
        "        inputs = [torch.LongTensor(i) for i in inputs]\n",
        "        tags = [torch.LongTensor(i) for i in tags]\n",
        "        masks = [torch.LongTensor(i) for i in masks]\n",
        "        mymasks = [torch.LongTensor(i) for i in mymasks]\n",
        "        dataloaders = []\n",
        "        for i,j,k,l in zip(inputs, tags, masks, mymasks):\n",
        "            data = TensorDataset(i, k, j, l)\n",
        "            sampler = RandomSampler(data)\n",
        "            dataloaders.append(DataLoader(data, sampler=sampler, batch_size=bs))\n",
        "\n",
        "        return dataloaders[0] if combine else dataloaders\n",
        "\n",
        "    def get_test_dataloader(self, val_size = 0.15, test_size=0.15, other_datasets = [], combine = True):\n",
        "        '''\n",
        "        This function returns the dataloader for the cue detection.\n",
        "        val_size: The size of the validation dataset (Fraction between 0 to 1)\n",
        "        test_size: The size of the test dataset (Fraction between 0 to 1)\n",
        "        other_datasets: Other datasets to use to get one combined train dataloader\n",
        "        Returns: train_dataloader, list of validation dataloaders, list of test dataloaders\n",
        "        '''\n",
        "        method = SCOPE_METHOD\n",
        "        do_lower_case = True\n",
        "        if 'uncased' not in SCOPE_MODEL and 'cased' in SCOPE_MODEL:\n",
        "            print(\"cased model\")\n",
        "            do_lower_case = False\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(SCOPE_MODEL, do_lower_case=do_lower_case)\n",
        "\n",
        "        def preprocess_data(obj):\n",
        "            dl_sents = obj.scope_data.sentences\n",
        "            dl_cues = obj.scope_data.cues\n",
        "            dl_scopes = obj.scope_data.scopes\n",
        "\n",
        "            sentences = [\" \".join([s for s in sent]) for sent in dl_sents]\n",
        "            mytexts = []\n",
        "            mylabels = []\n",
        "            mycues = []\n",
        "            mymasks = []\n",
        "            if do_lower_case == True:\n",
        "                sentences_clean = [sent.lower() for sent in sentences]\n",
        "            else:\n",
        "                sentences_clean = sentences\n",
        "\n",
        "            for sent, tags, cues in zip(sentences_clean,dl_scopes, dl_cues):\n",
        "                new_tags = []\n",
        "                new_text = []\n",
        "                new_cues = []\n",
        "                new_masks = []\n",
        "                for word, tag, cue in zip(sent.split(),tags,cues):\n",
        "                    sub_words = self.tokenizer.tokenize(word)\n",
        "                    for count, sub_word in enumerate(sub_words):\n",
        "                        mask = 1\n",
        "                        if count > 0:\n",
        "                            mask = 0\n",
        "                        new_masks.append(mask)\n",
        "                        new_tags.append(tag)\n",
        "                        new_cues.append(cue)\n",
        "                        new_text.append(sub_word)\n",
        "                mymasks.append(new_masks)\n",
        "                mytexts.append(new_text)\n",
        "                mylabels.append(new_tags)\n",
        "                mycues.append(new_cues)\n",
        "            final_sentences = []\n",
        "            final_labels = []\n",
        "            final_masks = []\n",
        "            if method == 'replace':\n",
        "                for sent,cues in zip(mytexts, mycues):\n",
        "                    temp_sent = []\n",
        "                    for token,cue in zip(sent,cues):\n",
        "                        if cue==3:\n",
        "                            temp_sent.append(token)\n",
        "                        else:\n",
        "                            temp_sent.append(f'[unused{cue+1}]')\n",
        "                    final_sentences.append(temp_sent)\n",
        "                final_labels = mylabels\n",
        "                final_masks = mymasks\n",
        "            elif method == 'augment':\n",
        "                for sent,cues,labels,masks in zip(mytexts, mycues, mylabels, mymasks):\n",
        "                    temp_sent = []\n",
        "                    temp_label = []\n",
        "                    temp_masks = []\n",
        "                    first_part = 0\n",
        "                    for token,cue,label,mask in zip(sent,cues,labels,masks):\n",
        "                        if cue!=3:\n",
        "                            if first_part == 0:\n",
        "                                first_part = 1\n",
        "                                temp_sent.append(f'[unused{cue+1}]')\n",
        "                                temp_masks.append(1)\n",
        "                                temp_label.append(label)\n",
        "                                temp_sent.append(token)\n",
        "                                temp_masks.append(0)\n",
        "                                temp_label.append(label)\n",
        "                                continue\n",
        "                            temp_sent.append(f'[unused{cue+1}]')\n",
        "                            temp_masks.append(0)\n",
        "                            temp_label.append(label)\n",
        "                        else:\n",
        "                            first_part = 0\n",
        "                        temp_masks.append(mask)\n",
        "                        temp_sent.append(token)\n",
        "                        temp_label.append(label)\n",
        "                    final_sentences.append(temp_sent)\n",
        "                    final_labels.append(temp_label)\n",
        "                    final_masks.append(temp_masks)\n",
        "                    if (len(temp_sent) > MAX_LEN):\n",
        "                      print(\"***WARNING*** \\ninput over MAX LENGTH\")\n",
        "            else:\n",
        "                raise ValueError(\"Supported methods for scope detection are:\\nreplace\\naugment\")\n",
        "            input_ids = pad_sequences([[self.tokenizer.convert_tokens_to_ids(word) for word in txt] for txt in final_sentences],\n",
        "                                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\").tolist()\n",
        "\n",
        "            tags = pad_sequences(final_labels,\n",
        "                                maxlen=MAX_LEN, value=0, padding=\"post\",\n",
        "                                dtype=\"long\", truncating=\"post\").tolist()\n",
        "\n",
        "            final_masks = pad_sequences(final_masks,\n",
        "                                maxlen=MAX_LEN, value=0, padding=\"post\",\n",
        "                                dtype=\"long\", truncating=\"post\").tolist()\n",
        "\n",
        "            attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "\n",
        "            return [input_ids, tags, attention_masks, final_masks]\n",
        "\n",
        "        inputs = []\n",
        "        tags = []\n",
        "        masks = []\n",
        "        mymasks = []\n",
        "        ret_val = preprocess_data(self)\n",
        "        inputs.append(ret_val[0])\n",
        "        tags.append(ret_val[1])\n",
        "        masks.append(ret_val[2])\n",
        "        mymasks.append(ret_val[3])\n",
        "\n",
        "        for idx, arg in enumerate(other_datasets):\n",
        "            ret_val = preprocess_data(arg)\n",
        "            if(combine):\n",
        "                inputs[0]+=ret_val[0]\n",
        "                tags[0]+=ret_val[1]\n",
        "                masks[0]+=ret_val[2]\n",
        "                mymasks[0]+=ret_val[3]\n",
        "            else:\n",
        "                inputs.append(ret_val[0])\n",
        "                tags.append(ret_val[1])\n",
        "                masks.append(ret_val[2])\n",
        "                mymasks.append(ret_val[3])\n",
        "\n",
        "\n",
        "        inputs = [torch.LongTensor(i) for i in inputs]\n",
        "        tags = [torch.LongTensor(i) for i in tags]\n",
        "        masks = [torch.LongTensor(i) for i in masks]\n",
        "        mymasks = [torch.LongTensor(i) for i in mymasks]\n",
        "        dataloaders = []\n",
        "        for i,k,l in zip(inputs, masks, mymasks):\n",
        "            data = TensorDataset(i, k, l)\n",
        "            sampler = RandomSampler(data)\n",
        "            dataloaders.append(DataLoader(data, sampler=sampler, batch_size=bs))\n",
        "\n",
        "        return (dataloaders[0], tags) if combine else (dataloaders, tags)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtDMtIXvG8BW"
      },
      "outputs": [],
      "source": [
        "def f1_cues(y_true, y_pred):\n",
        "    '''Needs flattened cues'''\n",
        "    tp = sum([1 for i,j in zip(y_true, y_pred) if (i==j and i!=3)])\n",
        "    fp = sum([1 for i,j in zip(y_true, y_pred) if (j!=3 and i==3)])\n",
        "    fn = sum([1 for i,j in zip(y_true, y_pred) if (i!=3 and j==3)])\n",
        "    if tp==0:\n",
        "        prec = 0.0001\n",
        "        rec = 0.0001\n",
        "    else:\n",
        "        prec = tp/(tp+fp)\n",
        "        rec = tp/(tp+fn)\n",
        "    print(f\"Precision: {prec}\")\n",
        "    print(f\"Recall: {rec}\")\n",
        "    print(f\"F1 Score: {2*prec*rec/(prec+rec)}\")\n",
        "    return prec, rec, 2*prec*rec/(prec+rec)\n",
        "\n",
        "\n",
        "def f1_scope(y_true, y_pred, level = 'token'): #This is for gold cue annotation scope, thus the precision is always 1.\n",
        "    if level == 'token':\n",
        "        print(f1_score([i for i in j for j in y_true], [i for i in j for j in y_pred]))\n",
        "    elif level == 'scope':\n",
        "        tp = 0\n",
        "        fn = 0\n",
        "        fp = 0\n",
        "        for y_t, y_p in zip(y_true, y_pred):\n",
        "            if y_t == y_p:\n",
        "                tp+=1\n",
        "            else:\n",
        "                fn+=1\n",
        "        prec = 1\n",
        "        rec = tp/(tp+fn)\n",
        "        print(f\"Precision: {prec}\")\n",
        "        print(f\"Recall: {rec}\")\n",
        "        print(f\"F1 Score: {2*prec*rec/(prec+rec)}\")\n",
        "\n",
        "def report_per_class_accuracy(y_true, y_pred):\n",
        "    labels = list(np.unique(y_true))\n",
        "    lab = list(np.unique(y_pred))\n",
        "    labels = list(np.unique(labels+lab))\n",
        "    n_labels = len(labels)\n",
        "    data = pd.DataFrame(columns = labels, index = labels, data = np.zeros((n_labels, n_labels)))\n",
        "    for i,j in zip(y_true, y_pred):\n",
        "        data.at[i,j]+=1\n",
        "    print(data)\n",
        "\n",
        "def flat_accuracy(preds, labels, input_mask = None):\n",
        "    pred_flat = [i for j in preds for i in j]\n",
        "    labels_flat = [i for j in labels for i in j]\n",
        "    return sum([1 if i==j else 0 for i,j in zip(pred_flat,labels_flat)]) / len(labels_flat)\n",
        "\n",
        "\n",
        "def flat_accuracy_positive_cues(preds, labels, input_mask = None):\n",
        "    pred_flat = [i for i,j in zip([i for j in preds for i in j],[i for j in labels for i in j]) if (j!=4 and j!=3)]\n",
        "    labels_flat = [i for i in [i for j in labels for i in j] if (i!=4 and i!=3)]\n",
        "    if len(labels_flat) != 0:\n",
        "        return sum([1 if i==j else 0 for i,j in zip(pred_flat,labels_flat)]) / len(labels_flat)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def scope_accuracy(preds, labels):\n",
        "    correct_count = 0\n",
        "    count = 0\n",
        "    for i,j in zip(preds, labels):\n",
        "        if i==j:\n",
        "            correct_count+=1\n",
        "        count+=1\n",
        "    return correct_count/count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYqvts8rHCkf"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 0\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation F1 increased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izQQ6sUdHEo3"
      },
      "outputs": [],
      "source": [
        "class ScopeModel:\n",
        "    def __init__(self, full_finetuning = True, train = False, pretrained_model_path = 'Scope_Resolution_Augment.pickle', device = 'cuda', learning_rate = 3e-5):\n",
        "        self.model_name = SCOPE_MODEL\n",
        "        self.task = TASK\n",
        "        self.num_labels = 2\n",
        "        if train == True:\n",
        "            self.model = AutoModelForTokenClassification.from_pretrained(SCOPE_MODEL, num_labels=self.num_labels)\n",
        "        else:\n",
        "            self.model = torch.load(pretrained_model_path)\n",
        "        self.device = torch.device(device)\n",
        "        if device=='cuda':\n",
        "            self.model.cuda()\n",
        "        else:\n",
        "            self.model.cpu()\n",
        "\n",
        "        if full_finetuning:\n",
        "            param_optimizer = list(self.model.named_parameters())\n",
        "            no_decay = ['bias', 'gamma', 'beta']\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                 'weight_decay_rate': 0.01},\n",
        "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                 'weight_decay_rate': 0.0}\n",
        "            ]\n",
        "        else:\n",
        "            param_optimizer = list(self.model.classifier.named_parameters())\n",
        "            optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "        self.optimizer = Adam(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "    def train(self, train_dataloader, valid_dataloaders, train_dl_name, val_dl_name, epochs = 5, max_grad_norm = 1.0, patience = 3):\n",
        "        self.train_dl_name = train_dl_name\n",
        "        return_dict = {\"Task\": f\"{self.task} Scope Resolution\",\n",
        "                       \"Model\": self.model_name,\n",
        "                       \"Train Dataset\": train_dl_name,\n",
        "                       \"Val Dataset\": val_dl_name,\n",
        "                       \"Best Precision\": 0,\n",
        "                       \"Best Recall\": 0,\n",
        "                       \"Best F1\": 0}\n",
        "        train_loss = []\n",
        "        valid_loss = []\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "        for _ in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "            self.model.train()\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_mymasks = batch\n",
        "                logits = self.model(b_input_ids,\n",
        "                             attention_mask=b_input_mask)[0]\n",
        "                active_loss = b_input_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss] #2 is num_labels\n",
        "                active_labels = b_labels.view(-1)[active_loss]\n",
        "                loss = loss_fn(active_logits, active_labels)\n",
        "                loss.backward()\n",
        "                tr_loss += loss.item()\n",
        "                train_loss.append(loss.item())\n",
        "                if step%100 == 0:\n",
        "                    print(f\"Batch {step}, loss {loss.item()}\")\n",
        "                nb_tr_examples += b_input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "                self.model.zero_grad()\n",
        "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "            eval_loss, eval_accuracy, eval_scope_accuracy = 0, 0, 0\n",
        "            nb_eval_steps, nb_eval_examples = 0, 0\n",
        "            predictions , true_labels, ip_mask = [], [], []\n",
        "            loss_fn = CrossEntropyLoss()\n",
        "            for valid_dataloader in valid_dataloaders:\n",
        "                for batch in valid_dataloader:\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    b_input_ids, b_input_mask, b_labels, b_mymasks = batch\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        logits = self.model(b_input_ids,\n",
        "                                      attention_mask=b_input_mask)[0]\n",
        "                        active_loss = b_input_mask.view(-1) == 1\n",
        "                        active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                        active_labels = b_labels.view(-1)[active_loss]\n",
        "                        tmp_eval_loss = loss_fn(active_logits, active_labels)\n",
        "\n",
        "                    logits = logits.detach().cpu().numpy()\n",
        "                    label_ids = b_labels.to('cpu').numpy()\n",
        "                    b_input_ids = b_input_ids.to('cpu').numpy()\n",
        "\n",
        "                    mymasks = b_mymasks.to('cpu').numpy()\n",
        "\n",
        "                    if F1_METHOD == 'first_token':\n",
        "\n",
        "                        logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
        "                        actual_logits = []\n",
        "                        actual_label_ids = []\n",
        "                        for l,lid,m in zip(logits, label_ids, mymasks):\n",
        "                            actual_logits.append([i for i,j in zip(l,m) if j==1])\n",
        "                            actual_label_ids.append([i for i,j in zip(lid, m) if j==1])\n",
        "\n",
        "                        logits = actual_logits\n",
        "                        label_ids = actual_label_ids\n",
        "\n",
        "                        predictions.append(logits)\n",
        "                        true_labels.append(label_ids)\n",
        "                    elif F1_METHOD == 'average':\n",
        "\n",
        "                        logits = [list(p) for p in logits]\n",
        "\n",
        "                        actual_logits = []\n",
        "                        actual_label_ids = []\n",
        "\n",
        "                        for l,lid,m,b_ii in zip(logits, label_ids, mymasks, b_input_ids):\n",
        "\n",
        "                            actual_label_ids.append([i for i,j in zip(lid, m) if j==1])\n",
        "                            my_logits = []\n",
        "                            curr_preds = []\n",
        "                            in_split = 0\n",
        "                            for i,j,k in zip(l,m, b_ii):\n",
        "                                '''if k == 0:\n",
        "                                    break'''\n",
        "                                if j==1:\n",
        "                                    if in_split == 1:\n",
        "                                        if len(my_logits)>0:\n",
        "                                            curr_preds.append(my_logits[-1])\n",
        "                                        mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                                        if len(my_logits)>0:\n",
        "                                            my_logits[-1] = mode_pred\n",
        "                                        else:\n",
        "                                            my_logits.append(mode_pred)\n",
        "                                        curr_preds = []\n",
        "                                        in_split = 0\n",
        "                                    my_logits.append(np.argmax(i))\n",
        "                                if j==0:\n",
        "                                    curr_preds.append(i)\n",
        "                                    in_split = 1\n",
        "                            if in_split == 1:\n",
        "                                if len(my_logits)>0:\n",
        "                                    curr_preds.append(my_logits[-1])\n",
        "                                mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                                if len(my_logits)>0:\n",
        "                                    my_logits[-1] = mode_pred\n",
        "                                else:\n",
        "                                    my_logits.append(mode_pred)\n",
        "                            actual_logits.append(my_logits)\n",
        "\n",
        "                        predictions.append(actual_logits)\n",
        "                        true_labels.append(actual_label_ids)\n",
        "\n",
        "                    tmp_eval_accuracy = flat_accuracy(actual_logits, actual_label_ids)\n",
        "                    tmp_eval_scope_accuracy = scope_accuracy(actual_logits, actual_label_ids)\n",
        "                    eval_scope_accuracy += tmp_eval_scope_accuracy\n",
        "                    valid_loss.append(tmp_eval_loss.mean().item())\n",
        "\n",
        "                    eval_loss += tmp_eval_loss.mean().item()\n",
        "                    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "                    nb_eval_examples += len(b_input_ids)\n",
        "                    nb_eval_steps += 1\n",
        "                eval_loss = eval_loss/nb_eval_steps\n",
        "            print(\"Validation loss: {}\".format(eval_loss))\n",
        "            print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "            print(\"Validation Accuracy Scope Level: {}\".format(eval_scope_accuracy/nb_eval_steps))\n",
        "            f1_scope([j for i in true_labels for j in i], [j for i in predictions for j in i], level='scope')\n",
        "            labels_flat = [l_ii for l in true_labels for l_i in l for l_ii in l_i]\n",
        "            pred_flat = [p_ii for p in predictions for p_i in p for p_ii in p_i]\n",
        "            classification_dict = classification_report(labels_flat, pred_flat, output_dict= True)\n",
        "            p = classification_dict[\"1\"][\"precision\"]\n",
        "            r = classification_dict[\"1\"][\"recall\"]\n",
        "            f1 = classification_dict[\"1\"][\"f1-score\"]\n",
        "            if f1>return_dict['Best F1']:\n",
        "                return_dict['Best F1'] = f1\n",
        "                return_dict['Best Precision'] = p\n",
        "                return_dict['Best Recall'] = r\n",
        "            print(\"F1-Score Token: {}\".format(f1))\n",
        "            print(classification_report(labels_flat, pred_flat))\n",
        "            print(f'F1: {f1}')\n",
        "            early_stopping(f1, self.model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Train Loss\")\n",
        "        plt.plot([i for i in range(len(train_loss))], train_loss)\n",
        "        plt.figure()\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Validation Loss\")\n",
        "        plt.plot([i for i in range(len(valid_loss))], valid_loss)\n",
        "        return return_dict\n",
        "\n",
        "    def evaluate(self, test_dataloader, test_dl_name, train_dl_name=\"pretrained\"):\n",
        "        return_dict = {\"Task\": f\"{self.task} Scope Resolution\",\n",
        "                       \"Model\": self.model_name,\n",
        "                       \"Train Dataset\": train_dl_name,\n",
        "                       \"Test Dataset\": test_dl_name,\n",
        "                       \"Precision\": 0,\n",
        "                       \"Recall\": 0,\n",
        "                       \"F1\": 0}\n",
        "        self.model.eval()\n",
        "        eval_loss, eval_accuracy, eval_scope_accuracy = 0, 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        predictions , true_labels, ip_mask = [], [], []\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_mymasks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(b_input_ids,\n",
        "                               attention_mask=b_input_mask)[0]\n",
        "                active_loss = b_input_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss] #5 is num_labels\n",
        "                active_labels = b_labels.view(-1)[active_loss]\n",
        "                tmp_eval_loss = loss_fn(active_logits, active_labels)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            b_input_ids = b_input_ids.to('cpu').numpy()\n",
        "\n",
        "            mymasks = b_mymasks.to('cpu').numpy()\n",
        "\n",
        "            if F1_METHOD == 'first_token':\n",
        "\n",
        "                logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
        "                actual_logits = []\n",
        "                actual_label_ids = []\n",
        "                for l,lid,m in zip(logits, label_ids, mymasks):\n",
        "                    actual_logits.append([i for i,j in zip(l,m) if j==1])\n",
        "                    actual_label_ids.append([i for i,j in zip(lid, m) if j==1])\n",
        "\n",
        "                logits = actual_logits\n",
        "                label_ids = actual_label_ids\n",
        "\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "\n",
        "            elif F1_METHOD == 'average':\n",
        "\n",
        "                logits = [list(p) for p in logits]\n",
        "\n",
        "                actual_logits = []\n",
        "                actual_label_ids = []\n",
        "\n",
        "                for l,lid,m,b_ii in zip(logits, label_ids, mymasks, b_input_ids):\n",
        "\n",
        "                    actual_label_ids.append([i for i,j in zip(lid, m) if j==1])\n",
        "                    my_logits = []\n",
        "                    curr_preds = []\n",
        "                    in_split = 0\n",
        "                    for i,j,k in zip(l,m,b_ii):\n",
        "                        '''if k == 0:\n",
        "                            break'''\n",
        "                        if j==1:\n",
        "                            if in_split == 1:\n",
        "                                if len(my_logits)>0:\n",
        "                                    curr_preds.append(my_logits[-1])\n",
        "                                mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                                if len(my_logits)>0:\n",
        "                                    my_logits[-1] = mode_pred\n",
        "                                else:\n",
        "                                    my_logits.append(mode_pred)\n",
        "                                curr_preds = []\n",
        "                                in_split = 0\n",
        "                            my_logits.append(np.argmax(i))\n",
        "                        if j==0:\n",
        "                            curr_preds.append(i)\n",
        "                            in_split = 1\n",
        "                    if in_split == 1:\n",
        "                        if len(my_logits)>0:\n",
        "                            curr_preds.append(my_logits[-1])\n",
        "                        mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                        if len(my_logits)>0:\n",
        "                            my_logits[-1] = mode_pred\n",
        "                        else:\n",
        "                            my_logits.append(mode_pred)\n",
        "                    actual_logits.append(my_logits)\n",
        "\n",
        "                predictions.append(actual_logits)\n",
        "                true_labels.append(actual_label_ids)\n",
        "\n",
        "            tmp_eval_accuracy = flat_accuracy(actual_logits, actual_label_ids)\n",
        "            tmp_eval_scope_accuracy = scope_accuracy(actual_logits, actual_label_ids)\n",
        "            eval_scope_accuracy += tmp_eval_scope_accuracy\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            nb_eval_examples += len(b_input_ids)\n",
        "            nb_eval_steps += 1\n",
        "        eval_loss = eval_loss/nb_eval_steps\n",
        "        print(\"Validation loss: {}\".format(eval_loss))\n",
        "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"Validation Accuracy Scope Level: {}\".format(eval_scope_accuracy/nb_eval_steps))\n",
        "        f1_scope([j for i in true_labels for j in i], [j for i in predictions for j in i], level='scope')\n",
        "        labels_flat = [l_ii for l in true_labels for l_i in l for l_ii in l_i]\n",
        "        pred_flat = [p_ii for p in predictions for p_i in p for p_ii in p_i]\n",
        "        classification_dict = classification_report(labels_flat, pred_flat, output_dict= True)\n",
        "        p = classification_dict[\"1\"][\"precision\"]\n",
        "        r = classification_dict[\"1\"][\"recall\"]\n",
        "        f1 = classification_dict[\"1\"][\"f1-score\"]\n",
        "        return_dict['Precision'] = p\n",
        "        return_dict['Recall'] = r\n",
        "        return_dict['F1'] = f1\n",
        "        return_dict['Mean Pred Scope Length'] = np.sum(pred_flat)/len(pred_flat)\n",
        "        return_dict['Mean Real Scope Length'] = np.sum(labels_flat)/len(labels_flat)\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(labels_flat, pred_flat))\n",
        "        return return_dict\n",
        "\n",
        "    def predict(self, dataloader):\n",
        "        self.model.eval()\n",
        "        predictions, ip_mask = [], []\n",
        "        for batch in dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_mymasks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(b_input_ids, attention_mask=b_input_mask)[0]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            mymasks = b_mymasks.to('cpu').numpy()\n",
        "\n",
        "            if F1_METHOD == 'first_token':\n",
        "\n",
        "                logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
        "                actual_logits = []\n",
        "                for l,lid,m in zip(logits, label_ids, mymasks):\n",
        "                    actual_logits.append([i for i,j in zip(l,m) if j==1])\n",
        "\n",
        "                logits = actual_logits\n",
        "                label_ids = actual_label_ids\n",
        "\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "\n",
        "            elif F1_METHOD == 'average':\n",
        "\n",
        "                logits = [list(p) for p in logits]\n",
        "\n",
        "                actual_logits = []\n",
        "\n",
        "                for l,m in zip(logits, mymasks):\n",
        "\n",
        "                    my_logits = []\n",
        "                    curr_preds = []\n",
        "                    in_split = 0\n",
        "                    for i,j in zip(l,m):\n",
        "\n",
        "                        if j==1:\n",
        "                            if in_split == 1:\n",
        "                                if len(my_logits)>0:\n",
        "                                    curr_preds.append(my_logits[-1])\n",
        "                                mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                                if len(my_logits)>0:\n",
        "                                    my_logits[-1] = mode_pred\n",
        "                                else:\n",
        "                                    my_logits.append(mode_pred)\n",
        "                                curr_preds = []\n",
        "                                in_split = 0\n",
        "                            my_logits.append(np.argmax(i))\n",
        "                        if j==0:\n",
        "                            curr_preds.append(i)\n",
        "                            in_split = 1\n",
        "                    if in_split == 1:\n",
        "                        if len(my_logits)>0:\n",
        "                            curr_preds.append(my_logits[-1])\n",
        "                        mode_pred = np.argmax(np.average(np.array(curr_preds), axis=0), axis=0)\n",
        "                        if len(my_logits)>0:\n",
        "                            my_logits[-1] = mode_pred\n",
        "                        else:\n",
        "                            my_logits.append(mode_pred)\n",
        "                    actual_logits.append(my_logits)\n",
        "\n",
        "                predictions.append(actual_logits)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from [huggingface](https://huggingface.co/datasets/rcds/MultiLegalNeg)"
      ],
      "metadata": {
        "id": "zCoSdt9AlaDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebqS93txHTsO"
      },
      "outputs": [],
      "source": [
        "de_dataset = load_dataset(\"rcds/MultiLegalNeg\", \"de\")\n",
        "fr_dataset = load_dataset(\"rcds/MultiLegalNeg\", \"fr\")\n",
        "it_dataset = load_dataset(\"rcds/MultiLegalNeg\", \"it\")\n",
        "ch_dataset = load_dataset(\"rcds/MultiLegalNeg\", \"swiss\")\n",
        "fr_dalloux = load_dataset(\"rcds/MultiLegalNeg\", \"fr_dalloux\")\n",
        "en_sfu = load_dataset(\"rcds/MultiLegalNeg\", \"en_sfu\")\n",
        "en_bioscope = load_dataset(\"rcds/MultiLegalNeg\", \"en_bioscope\")\n",
        "en_sherlock = load_dataset(\"rcds/MultiLegalNeg\", \"en_sherlock\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7s22vTNHYB5"
      },
      "outputs": [],
      "source": [
        "de_train = Data(de_dataset[\"train\"], dataset_name=\"de_train\")\n",
        "fr_train = Data(fr_dataset[\"train\"], dataset_name=\"fr_train\")\n",
        "it_train = Data(it_dataset[\"train\"], dataset_name=\"it_train\")\n",
        "ch_train = Data(ch_dataset[\"train\"], dataset_name=\"ch_train\")\n",
        "fr_dalloux_train = Data(fr_dalloux[\"train\"], dataset_name=\"fr_dalloux_train\")\n",
        "en_sfu_train = Data(en_sfu[\"train\"], dataset_name=\"en_sfu_train\")\n",
        "en_bioscope_train = Data(en_bioscope[\"train\"], dataset_name=\"en_bioscope_train\")\n",
        "en_sherlock_train =  Data(en_sherlock[\"train\"], dataset_name=\"en_sherlock_train\")\n",
        "\n",
        "de_val = Data(de_dataset[\"validation\"], dataset_name=\"de_val\")\n",
        "fr_val = Data(fr_dataset[\"validation\"], dataset_name=\"fr_val\")\n",
        "it_val = Data(it_dataset[\"validation\"], dataset_name=\"it_val\")\n",
        "ch_val = Data(ch_dataset[\"validation\"], dataset_name=\"ch_val\")\n",
        "fr_dalloux_val = Data(fr_dalloux[\"validation\"], dataset_name = \"fr_dalloux_val\")\n",
        "en_sfu_val = Data(en_sfu[\"validation\"], dataset_name=\"en_sfu_val\")\n",
        "en_bioscope_val = Data(en_bioscope[\"validation\"], dataset_name=\"en_bioscope_val\")\n",
        "en_sherlock_val =  Data(en_sherlock[\"validation\"], dataset_name=\"en_sherlock_val\")\n",
        "\n",
        "de_test = Data(de_dataset[\"test\"],dataset_name=\"de_test\")\n",
        "fr_test = Data(fr_dataset[\"test\"], dataset_name=\"fr_test\")\n",
        "it_test = Data(it_dataset[\"test\"], dataset_name=\"it_test\")\n",
        "ch_test = Data(ch_dataset[\"test\"], dataset_name=\"ch_test\")\n",
        "fr_dalloux_test = Data(fr_dalloux[\"test\"], dataset_name=\"fr_dalloux_test\")\n",
        "en_sfu_test = Data(en_sfu[\"test\"], dataset_name=\"en_sfu_test\")\n",
        "en_bioscope_test = Data(en_bioscope[\"test\"], dataset_name=\"en_bioscope_test\")\n",
        "en_sherlock_test =  Data(en_sherlock[\"test\"], dataset_name=\"en_sherlock_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnO-1hZ0HY9i"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    \"de\": {\"train\": de_train,\n",
        "           \"test\": de_test,\n",
        "           \"val\": de_val},\n",
        "    \"fr\": {\"train\": fr_train,\n",
        "           \"test\": fr_test,\n",
        "           \"val\": fr_val},\n",
        "    \"it\": {\"train\": it_train,\n",
        "           \"test\": it_test,\n",
        "           \"val\": it_val},\n",
        "    \"ch\": {\"train\": ch_train,\n",
        "           \"test\": ch_test,\n",
        "           \"val\": ch_val},\n",
        "    \"dalloux\": {\"train\": fr_dalloux_train,\n",
        "           \"test\": fr_dalloux_test,\n",
        "           \"val\": fr_dalloux_val},\n",
        "    \"sfu\": {\"train\": en_sfu_train,\n",
        "           \"test\": en_sfu_test,\n",
        "           \"val\": en_sfu_val},\n",
        "    \"sherlock\": {\"train\": en_sherlock_train,\n",
        "           \"test\": en_sherlock_test,\n",
        "           \"val\": en_sherlock_val},\n",
        "    \"bioscope\": {\"train\": en_bioscope_train,\n",
        "           \"test\": en_bioscope_test,\n",
        "           \"val\": en_bioscope_val}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train models"
      ],
      "metadata": {
        "id": "Px2D3Yu5lgBh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjaRnfPhGwAD"
      },
      "outputs": [],
      "source": [
        "#store results to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D78Xh9Wm1Os8"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/PATHTOFILE\"\n",
        "experiment_type = \"multilingual\" #multilingual or zero-shot\n",
        "train_datasets = [\"sfu\", \"bioscope\", \"sherlock\", \"dalloux\", \"de\", \"fr\", \"it\", \"ch\"]\n",
        "test_datasets = [\"fr\",\"it\", \"de\",\"ch\"]\n",
        "models = ['xlm-roberta-base',\n",
        "          'distilbert-base-multilingual-cased',\n",
        "          'bert-base-multilingual-uncased',\n",
        "          'xlm-roberta-large',\n",
        "          'cis-lmu/glot500-base',\n",
        "          \"joelito/legal-swiss-roberta-base\",\n",
        "          \"joelito/legal-swiss-roberta-large\",\n",
        "          \"joelito/legal-xlm-roberta-base\",\n",
        "          \"joelito/legal-xlm-roberta-large\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RwvU4o8Kyg9"
      },
      "outputs": [],
      "source": [
        "headers = [\"Task\", \"Model\", \"Train Dataset\", \"Test Dataset\",\"Precision\", \"Recall\", \"F1\", 'Mean Pred Scope Length', 'Mean Real Scope Length', \"Seed\"]\n",
        "f = open(file_path, \"a\")\n",
        "writer = csv.DictWriter(f, fieldnames = headers)\n",
        "writer.writeheader()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fqwFeOmzW39R"
      },
      "outputs": [],
      "source": [
        "for m in models:\n",
        "  print(f'training model {m}')\n",
        "  SCOPE_MODEL = m\n",
        "  model_results = []\n",
        "\n",
        "  for seed in range(1,6):\n",
        "    print(f'ROUND {seed}/5')\n",
        "    torch.manual_seed(seed)\n",
        "    t_ds = []\n",
        "    v_ds = []\n",
        "    tst_ds = []\n",
        "    test_dataloaders = {}\n",
        "    round_results = []\n",
        "\n",
        "    if experiment_type == \"zero_shot\":\n",
        "      for t in train_datasets:\n",
        "        t_ds.append(datasets[t][\"train\"])\n",
        "        t_ds.append(datasets[t][\"test\"])\n",
        "        v_ds.append(datasets[t][\"val\"])\n",
        "    elif experiment_type == \"multilingual\":\n",
        "      for t in train_datasets:\n",
        "        t_ds.append(datasets[t][\"train\"])\n",
        "        v_ds.append(datasets[t][\"val\"])\n",
        "\n",
        "    train_dl = t_ds[0].get_scope_dataloader(other_datasets = t_ds[1:], combine = True)\n",
        "    val_dls = v_ds[0].get_scope_dataloader(other_datasets = v_ds[1:], combine = False)\n",
        "\n",
        "    test_dls = []\n",
        "    for tst in test_datasets:\n",
        "      test_dls.append(datasets[tst][\"test\"].get_scope_dataloader())\n",
        "      test_dataloaders[tst] = test_dls[-1]\n",
        "\n",
        "\n",
        "    model = ScopeModel(full_finetuning=True, train=True, learning_rate = INITIAL_LEARNING_RATE)\n",
        "    result = model.train(train_dl, val_dls, epochs=EPOCHS, patience=PATIENCE, train_dl_name = \", \".join(train_datasets), val_dl_name = \", \".join(train_datasets))\n",
        "\n",
        "    for k in test_dataloaders.keys():\n",
        "      print(f\"Evaluate on {k}:\")\n",
        "      round_results.append(model.evaluate(test_dataloaders[k], test_dl_name = k))\n",
        "      round_results[-1][\"Seed\"] = seed\n",
        "\n",
        "    for i in round_results:\n",
        "      print(f'{i[\"Test Dataset\"]}: {i[\"F1\"]}')\n",
        "\n",
        "    model_results.append(round_results)\n",
        "    f = open(file_path, \"a\")\n",
        "    writer = csv.DictWriter(f, fieldnames = headers)\n",
        "    writer.writerows(round_results)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to pretty print annotatins and predictions"
      ],
      "metadata": {
        "id": "uDUl5-o9mTrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34hv-qJ3v97B"
      },
      "outputs": [],
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "def print_predictions(test_dataset, model): #model = trained ScopeModel\n",
        "    real_labels = []\n",
        "    pred_labels = []\n",
        "    count_corr = 0\n",
        "    count_false = 0\n",
        "    for i in test_dataset:\n",
        "      tst = Data([i])\n",
        "      if tst.scope_data.num_sentences>0:\n",
        "        tst_dl, tst_tags = tst.get_test_dataloader(combine=False)\n",
        "        preds = model.predict(tst_dl[0])\n",
        "        if preds[0] == tst.scope_data.scopes:\n",
        "          count_corr += 1\n",
        "        else:\n",
        "          count_false +=1\n",
        "        txt = \"\"\n",
        "        for idx, w in enumerate(tst.scope_data.sentences[0]):\n",
        "          real_labels.append(tst.scope_data.scopes[0][idx])\n",
        "          if tst.scope_data.scopes[0][idx] == 1:\n",
        "            txt += color.UNDERLINE\n",
        "          txt += w +\" \"+ color.END\n",
        "\n",
        "        print(f\"Annotation: {txt}\")\n",
        "        txt = \"\"\n",
        "        for idx, w in enumerate(tst.scope_data.sentences[0]):\n",
        "          pred_labels.append(preds[0][0][idx])\n",
        "          if preds[0][0][idx] == 1:\n",
        "            txt += color.UNDERLINE\n",
        "          txt += w +\" \"+ color.END\n",
        "\n",
        "        print(f\"Prediction: {txt}\")\n",
        "        print(\"---\")\n",
        "    print(f\"F1-score: {classification_report(real_labels, pred_labels, output_dict= True)['1']['f1-score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use pre-trained negation model**\n",
        "- load model from [huggingface](https://huggingface.co/rcds/neg-xlm-roberta-base)"
      ],
      "metadata": {
        "id": "4SGPlv9io4R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPE_MODEL = \"rcds/neg-xlm-roberta-base\"\n",
        "model = ScopeModel(full_finetuning=True, train=True, learning_rate = INITIAL_LEARNING_RATE)"
      ],
      "metadata": {
        "id": "UEEerTV1nXhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_predictions(de_dataset[\"test\"], model) #see example predictions"
      ],
      "metadata": {
        "id": "5M9fsR6KnhOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get full evaluation\n",
        "results = []\n",
        "test_dls = []\n",
        "test_dataloaders = {}\n",
        "test_datasets = [\"sfu\", \"bioscope\", \"sherlock\", \"dalloux\", \"de\", \"fr\", \"it\", \"ch\"]\n",
        "for tst in test_datasets:\n",
        "      test_dls.append(datasets[tst][\"test\"].get_scope_dataloader())\n",
        "      test_dataloaders[tst] = test_dls[-1]\n",
        "\n",
        "for k in test_dataloaders.keys():\n",
        "    print(f\"Evaluate on {k}:\")\n",
        "    results.append(model.evaluate(test_dataloaders[k], test_dl_name = k))\n",
        "    print(results[-1])\n"
      ],
      "metadata": {
        "id": "BpNNfBNQ9Jzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in results:\n",
        "  print(f\"{r['Test Dataset']}: {r['F1']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfbOMmXK_x2M",
        "outputId": "0f552fd1-51c5-474f-ef70-0a0bb3da2f7a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sfu: 0.8852874145353254\n",
            "bioscope: 0.955888415555861\n",
            "sherlock: 0.9047347404449515\n",
            "dalloux: 0.9399198931909212\n",
            "de: 0.9566343042071197\n",
            "fr: 0.9249183895538629\n",
            "it: 0.8881469115191986\n",
            "ch: 0.8781954887218045\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPZis0uOywpK3GOglf63dEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
